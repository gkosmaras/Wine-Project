{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, accuracy_score, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from pprint import pprint\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "red_wine=pd.read_csv(\"winequality-red.csv\",sep=\";\")\n",
    "white_wine=pd.read_csv(\"winequality-white.csv\",sep=\";\")\n",
    "#Column renaming according to usual conventions\n",
    "red_wine.columns=red_wine.columns.str.replace(\" \",\"_\")\n",
    "white_wine.columns=white_wine.columns.str.replace(\" \",\"_\")\n",
    "#Categorization of quality into three groups\n",
    "red_wine[\"quality_label\"] = red_wine[\"quality\"].apply(lambda value: 0 if value<=5 else 1 if value< 7 else 2)\n",
    "white_wine[\"quality_label\"]=white_wine[\"quality\"].apply(lambda value: 0 if value<=5 else 1 if value< 7 else 2)\n",
    "red_wine[\"quality_label\"]=pd.Categorical(red_wine[\"quality_label\"],categories=[0,1,2])\n",
    "white_wine[\"quality_label\"]=pd.Categorical(white_wine[\"quality_label\"],categories=[0,1,2])\n",
    "#Combining the two wine databases\n",
    "wines=pd.concat([red_wine,white_wine])\n",
    "wines=wines.sample(frac=1,random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=wines.drop([\"quality\",\"quality_label\"],axis=1)\n",
    "y=wines[\"quality\"]\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "xtrain_norm=xtrain\n",
    "xtest_norm=xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is normalised since distribution for all except pH is skewed\n",
    "#Normalisation only has a noticeable impact in non RF methods, and it's better with StandardScaler\n",
    "#However it results in lost of variance, concentrating most wines in one quality\n",
    "#If not normalised, the accuracy is less than 50%. With normalisation it it around 57%.\n",
    "norm=StandardScaler()\n",
    "norm_fit=norm.fit(xtrain)\n",
    "xtrain_norm=norm_fit.transform(xtrain)\n",
    "xtest_norm=norm_fit.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature visualization for RF\n",
    "temp=RandomForestClassifier(n_estimators=100)\n",
    "temp.fit(xtrain,ytrain)\n",
    "feature_imp=pd.Series(temp.feature_importances_,index=x.columns[:11]).sort_values(ascending=False)\n",
    "feature_imp.index=feature_imp.index.str.replace(\"_\",\" \")\n",
    "feature_imp.index=feature_imp.index.str.capitalize()\n",
    "sns.barplot(x=feature_imp,y=feature_imp.index,palette=\"rocket\",color=\"snow\").set_facecolor(color=\"snow\")\n",
    "plt.xlabel(\"Importance score\",fontsize=13)\n",
    "plt.ylabel(\"Features\",fontsize=13)\n",
    "plt.title(\"Feature importance visualization\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RandomizedSearch parameter tuning\n",
    "# n_estimators=[200,400,600,800,1000]\n",
    "# max_features=[\"auto\",\"sqrt\",\"log2\",0.3]\n",
    "# max_depth=[int(x) for x in np.linspace(10,100,num=10)]\n",
    "# max_depth.append(None)\n",
    "# min_samples_split=[1,2,3,4,5]\n",
    "# min_samples_leaf=[1,2,3,4]\n",
    "# bootstrap=[True,False]\n",
    "# grid={\"n_estimators\":n_estimators,\"max_features\":max_features,\"max_depth\":max_depth,\n",
    "# \"min_samples_split\":min_samples_split,\"min_samples_leaf\":min_samples_leaf,\"bootstrap\":bootstrap}\n",
    "# rf=RandomForestRegressor()\n",
    "# random_grid=RandomizedSearchCV(estimator=rf,param_distributions=grid,n_iter=100,cv=3,verbose=2,random_state=42,n_jobs=-1)\n",
    "# random_grid.fit(xtrain,ytrain)\n",
    "# print(random_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GridSearch parameter tuning\n",
    "# n_estimators=[500,600,700]\n",
    "# max_features=[\"auto\",\"log2\",0.3]\n",
    "# max_depth=[50,60,70]\n",
    "# min_samples_split=[1,2,3]\n",
    "# min_samples_leaf=[2,3,4]\n",
    "# bootstrap=[False,True]\n",
    "# best_grid={\"bootstrap\":bootstrap,\"max_depth\":max_depth,\"max_features\":max_features,\"min_samples_leaf\":min_samples_leaf,\n",
    "# \"min_samples_split\":min_samples_split,\"n_estimators\":n_estimators}\n",
    "# rf=RandomForestRegressor()\n",
    "# grid_search=GridSearchCV(estimator=rf,param_grid=best_grid,cv=3,n_jobs=-1,verbose=2)\n",
    "# grid_search.fit(xtrain_norm,ytrain)\n",
    "# print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamater for quality label depending on dataset:\n",
    "    #RED WINES\n",
    "strap=False\n",
    "depth=60\n",
    "features=\"log2\"\n",
    "samples_leaf=1\n",
    "samples_split=3\n",
    "estimators=900\n",
    "oob=False\n",
    "    #WHITE WINES\n",
    "strap=False\n",
    "depth=50\n",
    "features=\"log2\"\n",
    "samples_leaf=1\n",
    "samples_split=2\n",
    "estimators=700\n",
    "oob=False\n",
    "    #COMBINED\n",
    "strap=False\n",
    "depth=50\n",
    "features=\"log2\"\n",
    "samples_leaf=2\n",
    "samples_split=3\n",
    "estimators=700\n",
    "oob=False\n",
    "#Hyperparamater for quality depending on dataset:\n",
    "    #RED WINES\n",
    "strap=False\n",
    "depth=100\n",
    "features=\"sqrt\"\n",
    "samples_leaf=2\n",
    "samples_split=2\n",
    "estimators=1000\n",
    "oob=False\n",
    "    #WHITE WINES\n",
    "strap=False\n",
    "depth=50\n",
    "features=\"log2\"\n",
    "samples_leaf=2\n",
    "samples_split=3\n",
    "estimators=700\n",
    "oob=False\n",
    "    #COMBINED\n",
    "strap=False\n",
    "depth=60\n",
    "features=0.3\n",
    "samples_leaf=2\n",
    "samples_split=2\n",
    "estimators=600\n",
    "oob=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality_Label_Red=70%\n",
    "#Quality_Label_White=74%\n",
    "#Quality_Label_Combined=71%\n",
    "#Quality_Red=69%\n",
    "#Quality_White=70%\n",
    "#Quality_Combined=69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=estimators,max_depth=depth,max_features=features,\n",
    "min_samples_leaf=samples_leaf,min_samples_split=samples_split,bootstrap=strap,oob_score=oob,n_jobs=-1)\n",
    "rf.fit(xtrain_norm,ytrain)\n",
    "pred_rf=rf.predict(xtest_norm)\n",
    "a_rf=accuracy_score(ytest,pred_rf)\n",
    "print(a_rf)\n",
    "print(classification_report(ytest,pred_rf))\n",
    "plot_confusion_matrix(rf,xtest,ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regreession ~55%\n",
    "lr=LogisticRegression()\n",
    "lr.fit(xtrain_norm,ytrain)\n",
    "pred_lr=lr.predict(xtest_norm)\n",
    "a_lr=accuracy_score(ytest,pred_lr)\n",
    "print(a_lr)\n",
    "print(classification_report(ytest,pred_lr))\n",
    "plot_confusion_matrix(lr,xtest,ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC ~57%\n",
    "svc=SVC(C=1000)\n",
    "svc.fit(xtrain_norm,ytrain)\n",
    "pred_svc=svc.predict(xtest_norm)\n",
    "a_svc=accuracy_score(ytest,pred_svc)\n",
    "print(a_svc)\n",
    "print(classification_report(ytest,pred_svc))\n",
    "plot_confusion_matrix(svc,xtest,ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN ~58%\n",
    "knn=KNeighborsClassifier(n_neighbors=10,leaf_size=20)\n",
    "knn.fit(xtrain_norm,ytrain)\n",
    "pred_knn=knn.predict(xtest_norm)\n",
    "a_knn=accuracy_score(ytest,pred_knn)\n",
    "print(a_knn)\n",
    "print(classification_report(ytest,pred_knn))\n",
    "plot_confusion_matrix(knn,xtest,ytest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mods=pd.DataFrame({\"Ensemble\":[\"RandomForest\",\"LogisticRegression\",\"SupportVector\",\"KNeighbors\"],\"Accuracy\":[a_rf,a_lr,a_svc,a_knn]})\n",
    "mods.sort_values(by=\"Accuracy\",ascending=False,inplace=True)\n",
    "sns.barplot(x=\"Accuracy\",y=\"Ensemble\",data=mods).set_facecolor(color=\"snow\")\n",
    "plt.title(\"Comperative accuracy of different models\",fontsize=15)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34ab73eb2a201c4b750598ec4bd41f14c725bb809bc0c8207569acb80dee3ff5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
